{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a model with all the posts for an author (with at least 5 posts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import gzip\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoModel, BertModel\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, BertTokenizerFast, DataCollatorWithPadding, get_scheduler\n",
    "from torch.optim import AdamW\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from torch.nn.functional import cross_entropy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions\n",
    "# || Load the persona data || #\n",
    "def load_pickle(file_path):\n",
    "    with open(file_path, 'rb') as f:\n",
    "        persona_embeddings = pd.read_pickle(f)\n",
    "    return persona_embeddings\n",
    "\n",
    "def load_csv(file_path):\n",
    "    with open(file_path, 'rb') as f:\n",
    "        training_data = pd.read_csv(f)\n",
    "    return training_data\n",
    "\n",
    "def save_pickle(training_data, file_path):\n",
    "    with open(file_path, 'wb') as f:\n",
    "        pd.to_pickle(training_data, f)\n",
    "\n",
    "def save_csv(training_data, file_path):\n",
    "    with open(file_path, 'wb') as f:\n",
    "        training_data.to_csv(f)\n",
    "\n",
    "def create_persona_embeddings(grouped_data, model):\n",
    "    persona_embeddings = {}\n",
    "    for _, row in grouped_data.iterrows():\n",
    "        author = row['author_fullname']\n",
    "        posts = row['fulltext']\n",
    "        post_embeddings = model.encode(posts)\n",
    "        avg_embedding = np.mean(post_embeddings, axis=0)\n",
    "        persona_embeddings[author] = avg_embedding\n",
    "    return persona_embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Get data\n",
    "\n",
    "\n",
    "# # Load the persona data\n",
    "# data = load_pickle('../data/social_chemistry_posts.gzip')\n",
    "# data = data.dropna(subset=['author_fullname']) # Drop rows with missing author_fullname as then we can not create a persona for them\n",
    "\n",
    "# # keep only fulltext and author_fullname columns\n",
    "# data = data[['fulltext', 'author_fullname']]\n",
    "# # print(data)\n",
    "\n",
    "# # filter out authors with less than 5 posts\n",
    "# filtered_authors_counts = data['author_fullname'].value_counts()\n",
    "# filtered_authors = data[data['author_fullname'].isin(filtered_authors_counts[filtered_authors_counts >= 5].index)]\n",
    "# # print(filtered_authors)\n",
    "\n",
    "# # grouped by author_fullname\n",
    "# grouped = filtered_authors.groupby('author_fullname').agg(list).reset_index()\n",
    "# # print(grouped.head())\n",
    "\n",
    "\n",
    "# # create an embedding for each post and then average them to get the persona embedding\n",
    "\n",
    "\n",
    "# embedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "# author_embeddings = create_persona_embeddings(grouped, embedding_model)\n",
    "\n",
    "# print(len(author_embeddings[\"t2_17vpaz83\"]))\n",
    "\n",
    "# # Save the persona embeddings to a file\n",
    "# with open('../data/post_aggragate_embeddings.pkl', 'wb') as f:\n",
    "#     pd.to_pickle(author_embeddings, f)\n",
    "\n",
    "# print(\"Persona embeddings created and saved to ../data/post_aggragate_embeddings.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the persona embeddings from a file\n",
    "persona_embeddings = load_pickle('../data/post_aggragate_embeddings.pkl')\n",
    "\n",
    "# load training data\n",
    "training_data = load_csv('../data/social_comments_filtered.csv')\n",
    "\n",
    "# append the persona embeddings to the training data\n",
    "def append_persona_embeddings(training_data, persona_embeddings):\n",
    "    training_data['persona_embedding'] = training_data['author_fullname'].map(persona_embeddings)\n",
    "    return training_data\n",
    "\n",
    "training_data = append_persona_embeddings(training_data, persona_embeddings)\n",
    "\n",
    "# save the training data with persona embeddings to a file\n",
    "save_pickle(training_data, '../data/post_level_model/training_data.pkl')\n",
    "save_csv(training_data, '../data/post_level_model/training_data.csv')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentBertClassifier(nn.Module):\n",
    "    def __init__(self, use_embeddings=True, embedding_dim=384, \n",
    "                 num_outputs=2, sbert_dim=384, \n",
    "                 sbert_model='sentence-transformers/all-MiniLM-L6-v2'):\n",
    "        super().__init__()\n",
    "        print(\"Initializing with embedding input layer:\", use_embeddings)\n",
    "\n",
    "        self.model = AutoModel.from_pretrained(sbert_model)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        self.linear1 = nn.Linear(sbert_dim, sbert_dim // 2)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.use_embeddings = use_embeddings\n",
    "        \n",
    "        if use_embeddings:\n",
    "            # Combine SBERT + embedding vector\n",
    "            comb_in_dim = sbert_dim // 2 + embedding_dim\n",
    "            self.combine_linear = nn.Linear(comb_in_dim, comb_in_dim // 2)\n",
    "            self.output_layer = nn.Linear(comb_in_dim // 2, num_outputs)\n",
    "        else:\n",
    "            self.output_layer = nn.Linear(sbert_dim // 2, num_outputs)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, embedding_vector=None):\n",
    "        outputs = self.model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        pooled = outputs.last_hidden_state[:, 0]\n",
    "        x = self.relu(self.linear1(self.dropout(pooled)))\n",
    "\n",
    "        if self.use_embeddings and embedding_vector is not None:\n",
    "            x = torch.cat([x, embedding_vector], dim=1)\n",
    "            x = self.relu(self.combine_linear(x))\n",
    "\n",
    "        logits = self.output_layer(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data shape: (80, 4)\n",
      "Test data shape: (20, 4)\n",
      "      label                                               body  \\\n",
      "17251   NTA  NTA not only did you inform her about the movi...   \n",
      "25565   YTA  YTA You never made any kind of formal agreemen...   \n",
      "7859    YTA  YTA. You are a stay-at-home-mom. If it was agr...   \n",
      "12340   NTA  NTA That is not his business. I understand he ...   \n",
      "21171   YTA  YTA It can be really hard as a speaker to have...   \n",
      "\n",
      "      author_fullname                                  persona_embedding  \n",
      "17251     t2_1hu208zh  [-0.0154501535, 0.0719409, 0.006097397, -0.014...  \n",
      "25565     t2_2yhxhifg  [0.015882641, 0.034143534, 0.039112315, 0.0099...  \n",
      "7859      t2_17vpaz83  [0.022095518, 0.058831934, -0.03174363, -0.013...  \n",
      "12340        t2_ehvk2  [0.02064454, 0.024615703, 0.004179083, 0.02008...  \n",
      "21171        t2_ehvk2  [0.02064454, 0.024615703, 0.004179083, 0.02008...  \n",
      "torch.Size([80, 384])\n",
      "dict_keys(['input_ids', 'token_type_ids', 'attention_mask', 'persona_embedding', 'labels'])\n"
     ]
    }
   ],
   "source": [
    "# Load training data\n",
    "data = load_pickle('../data/post_level_model/training_data.pkl')\n",
    "# drop useless columns\n",
    "data = data.drop(columns=['id', 'permalink', 'parent_id', 'author_name'])\n",
    "# drop rows with missing values\n",
    "data = data.dropna(subset=['body', 'label', 'persona_embedding'])\n",
    "\n",
    "data = data[:100]\n",
    "\n",
    "# print(data.head())\n",
    "\n",
    "# create annotator model\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\") \n",
    "# print(device)\n",
    "\n",
    "# split data \n",
    "train_data, test_data = train_test_split(data, test_size=0.2, random_state=42)\n",
    "\n",
    "print(\"Train data shape:\", train_data.shape)\n",
    "print(\"Test data shape:\", test_data.shape)\n",
    "\n",
    "print(train_data.head())\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('sentence-transformers/all-MiniLM-L6-v2')\n",
    "\n",
    "# Tokenize the training data\n",
    "def tokenize_function(data):\n",
    "    tokenized = tokenizer(\n",
    "        data['body'].tolist(),\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=512,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "\n",
    "    tokenized[\"persona_embedding\"] = torch.tensor(data[\"persona_embedding\"].tolist())\n",
    "\n",
    "    tokenized[\"labels\"] = torch.tensor(\n",
    "        [1 if label == \"YTA\" else 0 for label in data[\"label\"].tolist()],\n",
    "        dtype=torch.long\n",
    "    )\n",
    "    return tokenized\n",
    "\n",
    "\n",
    "train_encodings = tokenize_function(train_data)\n",
    "test_encodings = tokenize_function(test_data)\n",
    "\n",
    "print(train_encodings[\"persona_embedding\"].shape)\n",
    "\n",
    "# print first row of the encodings\n",
    "print(train_encodings.keys())\n",
    "\n",
    "# convert to dataloaders\n",
    "train_dataset = torch.utils.data.TensorDataset(\n",
    "    train_encodings[\"input_ids\"],\n",
    "    train_encodings[\"attention_mask\"],\n",
    "    train_encodings[\"labels\"],\n",
    "    train_encodings[\"persona_embedding\"]\n",
    ")\n",
    "test_dataset = torch.utils.data.TensorDataset(\n",
    "    test_encodings[\"input_ids\"],\n",
    "    test_encodings[\"attention_mask\"],\n",
    "    test_encodings[\"labels\"],\n",
    "    test_encodings[\"persona_embedding\"]\n",
    ")\n",
    "\n",
    "BATCH_SIZE = 16\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "def train_model(model, train_dataloader, criterion, optimizer, num_epochs):\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        \n",
    "        for batch in train_dataloader:\n",
    "\n",
    "            # print(batch[0])\n",
    "            # print(batch[1])\n",
    "            # print(batch[2])\n",
    "            # print(batch[3])\n",
    "\n",
    "            input_ids = batch[0].to(device)\n",
    "            attention_mask = batch[1].to(device)\n",
    "            labels = batch[2].to(device)\n",
    "            persona_embedding = batch[3].to(device)\n",
    "            # print(input_ids.shape)\n",
    "            # print(attention_mask.shape)\n",
    "            # print(labels.shape)\n",
    "            # print(persona_embedding.shape)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(input_ids, attention_mask, embedding_vector=persona_embedding)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        \n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {total_loss / len(train_dataloader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model\n",
    "def evaluate_model(model, test_dataloader, criterion):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    true_labels = []\n",
    "    total_loss = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in test_dataloader:\n",
    "            input_ids = batch[0].to(device)\n",
    "            attention_mask = batch[1].to(device)\n",
    "            labels = batch[2].to(device)\n",
    "            persona_embedding = batch[3].to(device)\n",
    "\n",
    "            outputs = model(input_ids, attention_mask, embedding_vector=persona_embedding)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            logits = outputs.argmax(dim=1).cpu().numpy()\n",
    "            predictions.extend(logits)\n",
    "            true_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    # Calculate accuracy and F1 score\n",
    "    accuracy = accuracy_score(true_labels, predictions)\n",
    "    f1 = f1_score(true_labels, predictions, average='weighted')\n",
    "\n",
    "    print(f\"Test Loss: {total_loss / len(test_dataloader)}\")\n",
    "    print(f\"Accuracy: {accuracy}\")\n",
    "    print(f\"F1 Score: {f1}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 3\n",
    "LEARNING_RATE = 2e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing with embedding input layer: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "g:\\conda\\envs\\persona\\lib\\site-packages\\transformers\\optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3, Loss: 0.7102759003639221\n",
      "Epoch 2/3, Loss: 0.6865322709083557\n",
      "Epoch 3/3, Loss: 0.6606812238693237\n",
      "Test Loss: 0.6341192424297333\n",
      "Accuracy: 0.85\n",
      "F1 Score: 0.7810810810810811\n"
     ]
    }
   ],
   "source": [
    "# Train model with embeddings \n",
    "\n",
    "# Initialize the model\n",
    "model = SentBertClassifier(use_embeddings=True, embedding_dim=384, num_outputs=2, sbert_dim=384, sbert_model='sentence-transformers/all-MiniLM-L6-v2')\n",
    "model = model.to(device)\n",
    "\n",
    "# Loss and Optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = AdamW(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "train_model(model, train_dataloader, criterion, optimizer, EPOCHS)\n",
    "evaluate_model(model, test_dataloader, criterion)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing with embedding input layer: False\n",
      "Epoch 1/3, Loss: 0.6198644876480103\n",
      "Epoch 2/3, Loss: 0.5616430878639221\n",
      "Epoch 3/3, Loss: 0.49254134893417356\n",
      "Test Loss: 0.4147469401359558\n",
      "Accuracy: 0.85\n",
      "F1 Score: 0.7810810810810811\n"
     ]
    }
   ],
   "source": [
    "# Train model without embeddings\n",
    "# Initialize the model\n",
    "model_no_emb = SentBertClassifier(use_embeddings=False, num_outputs=2, sbert_dim=384, sbert_model='sentence-transformers/all-MiniLM-L6-v2')\n",
    "model_no_emb = model_no_emb.to(device)\n",
    "\n",
    "# Loss and Optimizer\n",
    "criterion_no_emb = nn.CrossEntropyLoss()\n",
    "optimizer_no_emb = AdamW(model_no_emb.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "train_model(model_no_emb, train_dataloader, criterion_no_emb, optimizer_no_emb, EPOCHS)\n",
    "evaluate_model(model_no_emb, test_dataloader, criterion_no_emb)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "persona",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
